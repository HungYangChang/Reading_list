# Reading_list
All the readings I am referring to for my master project. 

# PipeBERT (https://link.springer.com/article/10.1007/s11265-022-01814-y)
Abstract: Transformer-based models such as BERT model have achieved state-of-the-art accuracy in the natural language processing (NLP) tasks. Nevertheless, these models are extremely cumbersome and have low throughput in NLP inference. This is more challenging for edge inference due to the limited memory size and computational power of edge devices. Therefore, we aim to improve the edge inference throughput of transformer-based models, which is critical for real-life applications that process multiple independent tasks concurrently on resource-constrained devices to provide a better user experience. Pipelining a deep neural network (DNN) model across heterogeneous processing elements has been shown to significantly improve throughput. However, existing deep learning (DL) frameworks do not support pipeline inference, and previous works dedicated to pipelining lack full support for BERT models. In this work, we propose a heterogeneous pipelining framework (PipeBERT), built on TVM, for BERT models to utilize all available heterogeneous resources present in the ARM big.LITTLE architecture, which is common in modern edge devices. PipeBERT is the first pipelining framework that fully supports BERT operations, and improve overall throughput by employing heterogeneous ARM CPU clusters concurrently. PipeBERT splits BERT model into subgraphs, then maps subgraphs onto either ARM big or LITTLE cluster. To efficiently find pipeline configurations that balance the workload between heterogeneous clusters, we propose an improved binary search algorithm, which uses hardware performance metric feedback to find the best split configurations faster. Our search algorithm finds the best split point on average 1.2x and 165x faster than baseline binary search and exhaustive search, respectively. On the HiKey970 embedded platform and for BERT models, PipeBERT demonstrates on average 48.6% of higher inference throughput than running on four big cores (i.e., ARM big CPU cluster), and an average 61% of lower energy-delay product (EDP) than the best homogeneous inference.
